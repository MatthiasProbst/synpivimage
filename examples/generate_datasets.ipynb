{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import synpivimage as spi \n",
    "spi.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All particle images will be written to HDF5. We will use `h5rdmtoolbox` to explore the file content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5rdmtoolbox as h5tbx\n",
    "h5tbx.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Define an inital configuration\n",
    "The initial configuration shall be the default one of the synpivimage package. From there, we adjust the parameters.<br>\n",
    "Note, that the image width/height is chosen to be odd. This is recommended when using the auto-correlation feature later on.<br>\n",
    "Also note, that the default configuration **disabled noise generation** since it always can be added afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = spi.DEFAULT_CFG\n",
    "\n",
    "cfg['bit_depth'] = 8\n",
    "cfg['nx'] = 31\n",
    "cfg['ny'] = 31\n",
    "cfg['sensor_gain'] = 0.6\n",
    "cfg['particle_size_std'] = 1\n",
    "image_size = cfg['nx']*cfg['ny']\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining parameters to be varied\n",
    "\n",
    "Create tuples with the configuration name and the parameters. The parameter values can be a number only, a list or a numpy array (Obviously passing a number is equal to not specify the variation tuple at all).<br>\n",
    "Last we specify how many images per combinations will be generated with `n_per_combination`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variation_dict = {'particle_number': np.arange(1, image_size*0.1, 20).astype(int),\n",
    "                  'particle_size_mean': (2, 3),\n",
    "                  'laser_shape_factor': (1,10)}\n",
    "\n",
    "variation_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Generate a list of all config combinations:\n",
    "\n",
    "The `ConfigManager` takes the variable ranges to be varied and takes care of writing the data to (one or multipl) `HDF5` files.\n",
    "\n",
    "\n",
    "This class is initalize by passing a list of all configuration dictionaries that define each image generation. Thus we need 101 configuration dictionaries. Using `build_ConfgManger()` we retrieve the `ConfigManger`-instance by passing the inital config dictionary and the variation tuples from above (so we don't need to deal with building the man config dictionaries ourselves):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CFGs = spi.ConfigManager.from_variation_dict(initial_cfg=cfg,\n",
    "                                             variation_dict=variation_dict,\n",
    "                                             per_combination = 3,\n",
    "                                             shuffle=True)\n",
    "CFGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of configurations for the above example is computed like this: `5*2*2*3=60` where the integers are th length of lists in the `variation_dict` times the number of combination (`3`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write all data to HDF5 files:\n",
    "With a large amount of parameters the image generation may produce arrays that will blow your RAM. Therefore define the maximal number of images to be created before written to the file. This parameter is called `n_split`. If using `None`, all data is written into a single file. If you pass any integer number, then that number of files will be created. Filenames will look like `<dataset_dir>/ds_000001.hdf` and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hdf_filename = CFGs.generate(data_directory='example_data_dir', nproc=4, n_split=1000, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if all datasets have a correct `standard_name` based on the Standard Name Table on GitLab: https://git.scc.kit.edu/standard_name_tables/snt_particleimagevelocimetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5rdmtoolbox.conventions.cflike import StandardNameTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snt = StandardNameTable.from_gitlab(url='https://git.scc.kit.edu',\n",
    "                                    file_path='particle_image_velocimetry-v1.yaml',\n",
    "                                    project_id='35942',\n",
    "                                    ref_name='main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snt.check_file(hdf_filename[0], raise_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File(hdf_filename[0], 'r') as h5:\n",
    "    h5.dump()\n",
    "    h5['images'][0, :, :].plot(cmap='gray', size=2, aspect=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot particle image and labels together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with h5tbx.H5File(hdf_filename[0], 'r') as h5:\n",
    "    img_idx = 4\n",
    "    fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(8, 3))\n",
    "    h5['images'][img_idx, :, :].plot(cmap='gray', ax=ax[0], vmax=100)\n",
    "    h5['labels'][img_idx, :, :].plot(cmap='jet', ax=ax[1])\n",
    "    ax[0].set_aspect(1)\n",
    "    ax[1].set_aspect(1)\n",
    "    ax[0].set_title(f'image {img_idx}')\n",
    "    ax[1].set_title(f'label {img_idx}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File(hdf_filename[0], 'r') as h5:\n",
    "    h5['images'][0, :, :].plot.hist(bins=40, xlim=[0, 2**8], xscale='linear', yscale='log', figsize=(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02d57d4f82a712ded839b61a9a7962b0507366033663decc2eb4f0f687bfd0e6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
