{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.16'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import synpivimage as spi \n",
    "spi.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All particle images will be written to HDF5. We will use `h5rdmtoolbox` to explore the file content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5rdmtoolbox as h5tbx\n",
    "h5tbx.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Define an inital configuration\n",
    "The initial configuration shall be the default one of the synpivimage package. From there, we adjust the parameters.<br>\n",
    "Note, that the image width/height is chosen to be odd. This is recommended when using the auto-correlation feature later on.<br>\n",
    "Also note, that the default configuration **disabled noise generation** since it always can be added afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SynPivConfig(ny=31, nx=31, square_image=True, bit_depth=8, noise_baseline=0.0, dark_noise=0.0, shot_noise=False, sensitivity=1.0, qe=1.0, particle_number=1, particle_size_mean=2.5, particle_size_std=1, laser_width=3, laser_shape_factor=2, sensor_gain=0.6, particle_position_file=None, particle_size_illumination_dependency=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = spi.DEFAULT_CFG\n",
    "\n",
    "cfg['bit_depth'] = 8\n",
    "cfg['nx'] = 31\n",
    "cfg['ny'] = 31\n",
    "cfg['sensor_gain'] = 0.6\n",
    "cfg['particle_size_std'] = 1\n",
    "image_size = cfg['nx']*cfg['ny']\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining parameters to be varied\n",
    "\n",
    "Create tuples with the configuration name and the parameters. The parameter values can be a number only, a list or a numpy array (Obviously passing a number is equal to not specify the variation tuple at all).<br>\n",
    "Last we specify how many images per combinations will be generated with `n_per_combination`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'particle_number': array([ 1, 21, 41, 61, 81]),\n",
       " 'particle_size_mean': (2, 3),\n",
       " 'laser_shape_factor': (1, 10)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variation_dict = {\n",
    "    'particle_number': np.arange(1, image_size*0.1, 20).astype(int),\n",
    "    'particle_size_mean': (2, 3),\n",
    "    'laser_shape_factor': (1, 10)\n",
    "}\n",
    "\n",
    "variation_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Generate a list of all config combinations:\n",
    "\n",
    "The `ConfigManager` takes the variable ranges to be varied and takes care of writing the data to (one or multipl) `HDF5` files.\n",
    "\n",
    "\n",
    "This class is initalize by passing a list of all configuration dictionaries that define each image generation. Thus we need 101 configuration dictionaries. Using `build_ConfgManger()` we retrieve the `ConfigManger`-instance by passing the inital config dictionary and the variation tuples from above (so we don't need to deal with building the man config dictionaries ourselves):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConfigManager (20 configurations)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFGs = spi.ConfigManager.from_variation_dict(\n",
    "    initial_cfg=cfg,\n",
    "    variation_dict=variation_dict,\n",
    "    per_combination = 1,\n",
    "    shuffle=False\n",
    ")\n",
    "CFGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of configurations for the above example is computed like this: `5*2*2*3=60` where the integers are th length of lists in the `variation_dict` times the number of combination (`3`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write all data to HDF5 files:\n",
    "With a large amount of parameters the image generation may produce arrays that will blow your RAM. Therefore define the maximal number of images to be created before written to the file. This parameter is called `n_split`. If using `None`, all data is written into a single file. If you pass any integer number, then that number of files will be created. Filenames will look like `<dataset_dir>/ds_000001.hdf` and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 20 dataset into 1 HDF5 file(s). This may take a while...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                   | 0/20 [00:00<?, ?it/s]/home/ws/ht72/GitHub/synpivimage/synpivimage/core.py:301: UserWarning: Particles smaller then 0.1 are set to 0.1.\n",
      "  warnings.warn(f'Particles smaller then {PMIN_ALLOWED} are set to {PMIN_ALLOWED}.')\n",
      "/home/ws/ht72/GitHub/synpivimage/synpivimage/core.py:301: UserWarning: Particles smaller then 0.1 are set to 0.1.\n",
      "  warnings.warn(f'Particles smaller then {PMIN_ALLOWED} are set to {PMIN_ALLOWED}.')\n",
      "/home/ws/ht72/GitHub/synpivimage/synpivimage/core.py:301: UserWarning: Particles smaller then 0.1 are set to 0.1.\n",
      "  warnings.warn(f'Particles smaller then {PMIN_ALLOWED} are set to {PMIN_ALLOWED}.')\n",
      "/home/ws/ht72/GitHub/synpivimage/synpivimage/core.py:301: UserWarning: Particles smaller then 0.1 are set to 0.1.\n",
      "  warnings.warn(f'Particles smaller then {PMIN_ALLOWED} are set to {PMIN_ALLOWED}.')\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 688.25it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'bit_depth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m~/GitHub/synpivimage/synpivimage/core.py:579\u001b[0m, in \u001b[0;36mConfigManager.generate\u001b[0;34m(self, data_directory, particle_info, prefix, suffix, create_labels, overwrite, nproc, compression, compression_opts, n_split)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    567\u001b[0m              data_directory: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    568\u001b[0m              particle_info: Union[ParticleInfo, List[ParticleInfo], \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    575\u001b[0m              compression_opts: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m    576\u001b[0m              n_split: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[pathlib\u001b[38;5;241m.\u001b[39mPath]:\n\u001b[1;32m    577\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"returns the generated data (intensities and particle information)\u001b[39;00m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;124;03m    This will not return all particle image information. Only number of particles!\"\"\"\u001b[39;00m\n\u001b[0;32m--> 579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_and_store_in_hdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mparticle_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparticle_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mcreate_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mnproc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnproc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mcompression_opts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression_opts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mn_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_split\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/synpivimage/synpivimage/core.py:787\u001b[0m, in \u001b[0;36mConfigManager._generate_and_store_in_hdf\u001b[0;34m(self, data_directory, prefix, suffix, particle_info, create_labels, overwrite, nproc, compression, compression_opts, n_split)\u001b[0m\n\u001b[1;32m    784\u001b[0m ds_std_size[:] \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mstd(p\u001b[38;5;241m.\u001b[39msize) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m particle_information]\n\u001b[1;32m    785\u001b[0m \u001b[38;5;66;03m# ds_intensity_mean[:] = [np.mean(p['intensity']) for p in particle_information]\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# ds_intensity_std[:] = [np.std(p['intensity']) for p in particle_information]\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m ds_bitdepth[:] \u001b[38;5;241m=\u001b[39m [a\u001b[38;5;241m.\u001b[39mbit_depth \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attrs]\n\u001b[1;32m    789\u001b[0m part_pos_grp \u001b[38;5;241m=\u001b[39m h5\u001b[38;5;241m.\u001b[39mcreate_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparticle_infos\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ipart, part_info \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(particle_information):\n",
      "File \u001b[0;32m~/GitHub/synpivimage/synpivimage/core.py:787\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    784\u001b[0m ds_std_size[:] \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mstd(p\u001b[38;5;241m.\u001b[39msize) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m particle_information]\n\u001b[1;32m    785\u001b[0m \u001b[38;5;66;03m# ds_intensity_mean[:] = [np.mean(p['intensity']) for p in particle_information]\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# ds_intensity_std[:] = [np.std(p['intensity']) for p in particle_information]\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m ds_bitdepth[:] \u001b[38;5;241m=\u001b[39m [\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbit_depth\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attrs]\n\u001b[1;32m    789\u001b[0m part_pos_grp \u001b[38;5;241m=\u001b[39m h5\u001b[38;5;241m.\u001b[39mcreate_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparticle_infos\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ipart, part_info \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(particle_information):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'bit_depth'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hdf_filename = CFGs.generate(data_directory='example_data_dir', nproc=4, n_split=1000, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0010405827263267429"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/31/31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hdf_filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5tbx\u001b[38;5;241m.\u001b[39mFile(\u001b[43mhdf_filename\u001b[49m[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m h5:\n\u001b[1;32m      2\u001b[0m     h5\u001b[38;5;241m.\u001b[39mdump()\n\u001b[1;32m      3\u001b[0m     h5[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnparticles\u001b[39m\u001b[38;5;124m'\u001b[39m][:]\u001b[38;5;241m.\u001b[39mplot()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hdf_filename' is not defined"
     ]
    }
   ],
   "source": [
    "with h5tbx.File(hdf_filename[0], 'r') as h5:\n",
    "    h5.dump()\n",
    "    h5['nparticles'][:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.File(hdf_filename[0], 'r') as h5:\n",
    "    h5.dump()\n",
    "    h5['images'][0, :, :].plot(cmap='gray', size=2, aspect=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot particle image and labels together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with h5tbx.File(hdf_filename[0], 'r') as h5:\n",
    "    img_idx = 4\n",
    "    fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(8, 3))\n",
    "    h5['images'][img_idx, :, :].plot(cmap='gray', ax=ax[0], vmax=100)\n",
    "    h5['labels'][img_idx, :, :].plot(cmap='jet', ax=ax[1])\n",
    "    ax[0].set_aspect(1)\n",
    "    ax[1].set_aspect(1)\n",
    "    ax[0].set_title(f'image {img_idx}')\n",
    "    ax[1].set_title(f'label {img_idx}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.File(hdf_filename[0], 'r') as h5:\n",
    "    h5['images'][0, :, :].plot.hist(bins=40, xlim=[0, 2**8], xscale='linear', yscale='log', figsize=(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02d57d4f82a712ded839b61a9a7962b0507366033663decc2eb4f0f687bfd0e6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
