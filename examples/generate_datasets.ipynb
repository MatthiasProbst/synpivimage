{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package for creating synthetic PIV images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import synpivimage as spi \n",
    "spi.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5rdmtoolbox as h5tbx\n",
    "h5tbx.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the notebook we retrieve the dataset directory from the dot-env file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv('generation.env')\n",
    "dataset_dir = os.getenv('dataset_dir')\n",
    "dataset_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Define an inital configuration\n",
    "The initial configuration shall be the default one of the synpivimage package. From there, we adjust the parameters.<br>\n",
    "Note, that the image width/height is chosen to be odd. This is recommended when using the auto-correlation feature later on.<br>\n",
    "Also note, that the default configuration **disabled noise generation** since it always can be added afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = spi.DEFAULT_CFG\n",
    "\n",
    "cfg['bit_depth'] = 8\n",
    "cfg['nx'] = 32\n",
    "cfg['ny'] = 32\n",
    "cfg['sensor_gain'] = 0.6\n",
    "cfg['particle_size_std'] = 1\n",
    "image_size = cfg['nx']*cfg['ny']\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining parameters to be varied\n",
    "\n",
    "Create tuples with the configuration name and the parameters. The parameter values can be a number only, a list or a numpy array (Obviously passing a number is equal to not specify the variation tuple at all).<br>\n",
    "Last we specify how many images per combinations will be generated with `n_per_combination`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# particle_number_range = ('particle_number', np.arange(1, image_size*0.1, 10).astype(int))\n",
    "particle_number_range = ('particle_number', int(0.1*30*30))\n",
    "particle_mean_size_range = ('particle_size_mean', (2, 3))\n",
    "lasershape_range = ('laser_shape_factor', (1,10))\n",
    "\n",
    "n_per_combination = 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Generate a list of all config combinations:\n",
    "Before we can write the data to disk, we generate a `ConfigManager`. This class is initalize by passing a list of all configuration dictionaries that define each image generation. Thus we need 101 configuration dictionaries. Using `build_ConfgManger()` we retrieve the `ConfigManger`-instance by passing the inital config dictionary and the variation tuples from above (so we don't need to deal with building the man config dictionaries ourselves):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CFGs = spi.build_ConfigManager(cfg, [particle_number_range,\n",
    "                                     particle_mean_size_range,\n",
    "                                     lasershape_range],\n",
    "                              per_combination = n_per_combination,\n",
    "                              shuffle=True)\n",
    "CFGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write all data to HDF5 files:\n",
    "With a large amount of parameters the image generation may produce arrays that will blow your RAM. Therefore define the maximal number of images to be created before written to the file. This parameter is called `n_split`. If using `None`, all data is written into a single file. If you pass any integer number, then that number of files will be created. Filenames will look like `<dataset_dir>/ds_000001.hdf` and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hdf_filename = CFGs.to_hdf(dataset_dir, nproc=1, n_split=1000, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if all datasets have a correct `standard_name` based on the Standard Name Table on GitLab: https://git.scc.kit.edu/standard_name_tables/snt_particleimagevelocimetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ureg = pint.UnitRegistry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ureg.pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snt = h5tbx.conventions.cflike.standard_name.StandardNameTable.from_gitlab(url='https://git.scc.kit.edu',\n",
    "                                                                           file_path='particle_image_velocimetry-v1.yaml',\n",
    "                                                                           project_id='35942',\n",
    "                                                                           ref_name='main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snt.check_file(hdf_filename[0], raise_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File(hdf_filename[0], 'r') as h5:\n",
    "    h5.dump()\n",
    "    h5['images'][0, :, :].plot(cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5tbx.H5File(hdf_filename[0], 'r') as h5:\n",
    "    h5['images'][0, :, :].plot.hist(bins=40, xlim=[0, 2**8], xscale='linear', yscale='linear')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02d57d4f82a712ded839b61a9a7962b0507366033663decc2eb4f0f687bfd0e6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
